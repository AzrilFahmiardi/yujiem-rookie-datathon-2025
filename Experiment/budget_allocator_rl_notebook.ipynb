{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65d7470",
   "metadata": {},
   "source": [
    "\n",
    "# Budget Allocator â€” RL Prototype (Colab-ready)\n",
    "\n",
    "Notebook contents:\n",
    "- Synthetic influencer dataset generator\n",
    "- Gym-style `BudgetAllocEnv` environment (step, reset, observation)\n",
    "- Training snippet using `stable-baselines3` (PPO)\n",
    "- Baseline: greedy heuristic and MIP (sketch using `pulp`)\n",
    "- Evaluation utilities and tips for running on Colab\n",
    "\n",
    "**How to use**\n",
    "1. Upload this notebook to Google Colab.\n",
    "2. Uncomment and run the `!pip install` cell to install dependencies (`gym`, `stable-baselines3`, `pulp`).\n",
    "3. Run cells step-by-step. Training cell provided with modest timesteps for quick prototyping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment and run in Colab (or local env) to install required packages\n",
    "# !pip install gym==0.26.5 stable-baselines3[extra]==2.0.0 pulp numpy==1.23.0\n",
    "# If running on local machine, ensure compatible versions of gym and sb3 are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment and synthetic data generator\n",
    "import gym, numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "def make_influencers(N=12, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    infs = []\n",
    "    for i in range(N):\n",
    "        tier = np.random.choice(['micro','mid','macro'], p=[0.6,0.3,0.1])\n",
    "        if tier == 'micro':\n",
    "            base = 80_000\n",
    "            reach_base = 2_000\n",
    "        elif tier == 'mid':\n",
    "            base = 300_000\n",
    "            reach_base = 20_000\n",
    "        else:\n",
    "            base = 1_200_000\n",
    "            reach_base = 100_000\n",
    "        infs.append({\n",
    "            \"id\": i,\n",
    "            \"tier\": tier,\n",
    "            \"rate\": {\"story\": int(base*0.2), \"feed\": int(base), \"reels\": int(base*1.4)},\n",
    "            \"base_reach\": {\"story\": reach_base*0.2, \"feed\": reach_base, \"reels\": int(reach_base*1.2)},\n",
    "            \"engagement_rate\": float(np.clip(np.random.normal(0.03, 0.008), 0.005, 0.2)),\n",
    "            \"audience_match\": float(np.clip(np.random.beta(2,2), 0.0, 1.0)),\n",
    "            \"authenticity\": float(np.clip(np.random.beta(3,2), 0.0, 1.0)),\n",
    "            \"variability\": float(np.random.uniform(0.05, 0.25))\n",
    "        })\n",
    "    return infs\n",
    "\n",
    "class BudgetAllocEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Discrete-action environment for budget allocation.\n",
    "    Action: Discrete encoding: influencer_idx * (formats*counts) + fmt*counts + cnt\n",
    "    formats = 3 (story, feed, reels), counts = 3 (0,1,2 posts)\n",
    "    \"\"\"\n",
    "    def __init__(self, influencers, budget, objective='conversion', max_steps=8):\n",
    "        super().__init__()\n",
    "        self.influencers = influencers\n",
    "        self.N = len(influencers)\n",
    "        self.budget = float(budget)\n",
    "        self.objective = objective\n",
    "        self.max_steps = max_steps\n",
    "        self.formats = ['story','feed','reels']\n",
    "        self.count_buckets = [0,1,2]  # posts per chosen action\n",
    "\n",
    "        # features per influencer: rates(3), base_reaches(3), engagement_rate, audience_match, authenticity => 9\n",
    "        F = 9\n",
    "        obs_dim = 1 + self.N * F + 1  # budget_left + flattened influencer feats + objective id (or steps left)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.N * len(self.formats) * len(self.count_buckets))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.budget_left = float(self.budget)\n",
    "        self.steps = 0\n",
    "        # optionally track allocations\n",
    "        self.allocations = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        feats = []\n",
    "        for inf in self.influencers:\n",
    "            feats.extend([\n",
    "                inf['rate']['story'] / (self.budget + 1),\n",
    "                inf['rate']['feed'] / (self.budget + 1),\n",
    "                inf['rate']['reels'] / (self.budget + 1),\n",
    "                inf['base_reach']['story'] / 1e6,\n",
    "                inf['base_reach']['feed'] / 1e6,\n",
    "                inf['base_reach']['reels'] / 1e6,\n",
    "                inf['engagement_rate'],\n",
    "                inf['audience_match'],\n",
    "                inf['authenticity']\n",
    "            ])\n",
    "        obj_id = 0 if self.objective == 'awareness' else (1 if self.objective=='conversion' else 2)\n",
    "        obs = np.concatenate([[self.budget_left / (self.budget+1)], np.array(feats), [obj_id]])\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        inf_idx = action // (len(self.formats)*len(self.count_buckets))\n",
    "        rem = action % (len(self.formats)*len(self.count_buckets))\n",
    "        fmt = rem // len(self.count_buckets)\n",
    "        cnt = rem % len(self.count_buckets)  # 0..2\n",
    "\n",
    "        inf = self.influencers[int(inf_idx)]\n",
    "        format_name = self.formats[int(fmt)]\n",
    "        cost = inf['rate'][format_name] * int(cnt)\n",
    "\n",
    "        if cnt == 0:\n",
    "            # no-op selection -> small negative to discourage wasted step\n",
    "            reward = -0.01\n",
    "            info = {\"valid\": True, \"cost\": 0.0}\n",
    "        elif cost > self.budget_left:\n",
    "            # invalid over-budget action -> negative reward and no budget change\n",
    "            reward = -0.2\n",
    "            info = {\"valid\": False, \"cost\": cost}\n",
    "        else:\n",
    "            # simulate outcomes\n",
    "            base_reach = inf['base_reach'][format_name] * cnt\n",
    "            noise = np.random.normal(1.0, inf['variability'])\n",
    "            reach = max(0.0, base_reach * noise)\n",
    "            engagements = reach * inf['engagement_rate']\n",
    "            effective_eng = engagements * inf['audience_match']\n",
    "            conversions = effective_eng * 0.002  # example conversion rate per engagement\n",
    "            if self.objective == 'awareness':\n",
    "                reward = np.log1p(reach) / 10.0\n",
    "            elif self.objective == 'conversion':\n",
    "                reward = conversions * 100.0 + effective_eng * 0.01\n",
    "            else:\n",
    "                reward = (conversions * 100.0) / (cost + 1.0)\n",
    "\n",
    "            self.budget_left -= cost\n",
    "            self.allocations.append({\"inf\": inf['id'], \"format\": format_name, \"count\": cnt, \"cost\": cost})\n",
    "\n",
    "            info = {\"valid\": True, \"cost\": cost, \"reach\": reach, \"engagements\": engagements, \"conversions\": conversions}\n",
    "\n",
    "        self.steps += 1\n",
    "        done = (self.budget_left <= 0) or (self.steps >= self.max_steps)\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), bool(done), info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(\"Budget left:\", self.budget_left, \"Allocations:\", self.allocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training snippet (run after installing stable-baselines3)\n",
    "# This cell uses PPO from stable-baselines3. For quick tests, reduce total_timesteps.\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "except Exception as e:\n",
    "    print(\"stable-baselines3 not available. Please install in Colab: !pip install stable-baselines3[extra]\")\n",
    "    raise\n",
    "\n",
    "# create synthetic influencers and env\n",
    "infs = make_influencers(N=12, seed=42)\n",
    "env = DummyVecEnv([lambda: BudgetAllocEnv(infs, budget=1_000_000, objective='conversion', max_steps=8)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=3e-4, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99)\n",
    "# For prototyping, keep timesteps low (e.g., 50k). Increase for better policies.\n",
    "model.learn(total_timesteps=50000)\n",
    "model.save(\"ppo_budget_alloc_prototype\")\n",
    "print(\"Training finished, model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline: simple greedy heuristic and MIP sketch\n",
    "def greedy_baseline(influencers, budget, objective='conversion'):\n",
    "    remaining = budget\n",
    "    allocations = []\n",
    "    # score = expected_value_per_cost = (expected_conversions * value_per_conv)/cost approximated\n",
    "    for inf in sorted(influencers, key=lambda x: (x['audience_match'] * x['engagement_rate'] * x['base_reach']['feed'])/x['rate']['feed'], reverse=True):\n",
    "        # try choose feed first, then reels, then story\n",
    "        for fmt in ['feed','reels','story']:\n",
    "            cnt = 0\n",
    "            while True:\n",
    "                cost = inf['rate'][fmt]\n",
    "                if cost <= remaining and cnt < 2:  # limit posts to 2 in baseline\n",
    "                    remaining -= cost\n",
    "                    allocations.append({\"inf\": inf['id'], \"format\": fmt, \"count\": 1, \"cost\": cost})\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    break\n",
    "    return allocations\n",
    "\n",
    "# MIP sketch (deterministic expected reward) - requires pulp\n",
    "mip_sketch = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973047ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation helpers\n",
    "def simulate_allocations(influencers, allocations):\n",
    "    total_cost = 0.0\n",
    "    total_reach = 0.0\n",
    "    total_eng = 0.0\n",
    "    total_conv = 0.0\n",
    "    for a in allocations:\n",
    "        inf = influencers[a['inf']]\n",
    "        fmt = a['format']\n",
    "        cnt = a['count']\n",
    "        cost = inf['rate'][fmt] * cnt\n",
    "        base_reach = inf['base_reach'][fmt] * cnt\n",
    "        noise = np.random.normal(1.0, inf['variability'])\n",
    "        reach = max(0.0, base_reach * noise)\n",
    "        eng = reach * inf['engagement_rate']\n",
    "        eff = eng * inf['audience_match']\n",
    "        conv = eff * 0.002\n",
    "        total_cost += cost\n",
    "        total_reach += reach\n",
    "        total_eng += eng\n",
    "        total_conv += conv\n",
    "    return {\"cost\": total_cost, \"reach\": total_reach, \"eng\": total_eng, \"conv\": total_conv}\n",
    "\n",
    "# Quick test of greedy baseline\n",
    "infs = make_influencers(12, seed=1)\n",
    "alloc = greedy_baseline(infs, budget=1_000_000)\n",
    "res = simulate_allocations(infs, alloc)\n",
    "print(\"Greedy baseline result:\", res, \"alloc len:\", len(alloc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a2f20",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Notes & Tips\n",
    "- The notebook uses a simple discrete action encoding for clarity. For improved performance, consider:\n",
    "  - Action masking to prevent over-budget actions.\n",
    "  - Hierarchical policies (pick influencer â†’ pick format â†’ pick count).\n",
    "  - Continuous allocation output (fractions of budget) with post-processing / rounding.\n",
    "- Reward shaping is crucial: tune constants so PPO receives stable, non-sparse feedback.\n",
    "- Use curriculum learning: start with generous budgets and low stochasticity, then increase difficulty.\n",
    "- Compare RL vs MIP baseline on identical seeds and average results across many episodes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
